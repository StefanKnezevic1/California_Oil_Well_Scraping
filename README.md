# Scraping-California-Oil-Well-Info
This was my first coding project. Throughout this process I familiarized myself with Python and learned about different libraries available to me. I used Selenium webdriver and the CSV library to scan through API numbers for oil wells, scrape the relevant data from those websites and write it all into a concise excel document. The CSV file attached to this repository is a snippet of the generated data. The full dataset has ~115k records and is too big to upload to GitHub.

This will scrape all of the California data. It scans through all of the counties and increases the ID value of the API number until there are three consecutive failed API searches, whereby it switches to the next county. The excel file contains all the scraped data from using this program. Although this scrapes every document in the California repository, it only grabbed the first line of the casing data, and the completion info. Some of these documents carry a large amount of casing information but it is impossible for me to grab that info considering how different the structure is document to document. If the document contains casing info, I would predict a 50% chance there is more casing info to be found.

